shutting down VB and hadoop


goto http://localhost:8080/#/main/hosts
select the virtual machine.
then goto action menu and select stop all components
use same to restart all but restart hdfs first

Ambari:: http://127.0.0.1:8080/#/main/dashboard/metrics

Sandbox Welcome Page	http://127.0.0.1:8888
Ambari Dashboard		http://127.0.0.1:8080
ManageAmbari			http://127.0.0.1:8080/views/ADMIN_VIEW/2.4.0.0/INSTANCE/#/
Hive User View			http://127.0.0.1:8080/#/main/views/HIVE/1.5.0/AUTO_HIVE_INSTANCE
Pig User View			http://127.0.0.1:8080/#/main/views/PIG/1.0.0/Pig_INSTANCE
File User View			http://127.0.0.1:8080/#/main/views/FILES/1.0.0/AUTO_FILES_INSTANCE
SSH Web Client			http://127.0.0.1:4200
Hadoop Configuration	http://127.0.0.1:50070/dfshealth.html http://127.0.0.1:50070/explorer.html

SSH inot host

127.0.0.1  2222

username root initial password : hadoop
password Ramandeep

commands usefull
ifconfig: as ipconfig
sandbox-version:to know sandbox version
sudo ambari-admin-password-reset:to reset admin password for ambari.
ambari-agent restart:to restart ambari




Spark 2
Zeppelin workbook
http://sandbox.hortonworks.com:9995

creating hive context

%jdbc(hive) show tables-->query to list all the tables--

%spark2
val hiveContext = new org.apache.spark.sql.SparkSession.Builder().getOrCreate()		--->query to create hive context


%spark2
hiveContext.sql("show tables").show() --->>get list of tables in hive using hive context

as drivermileage and truck mileage are there in hive Now we create RDD(resillient distributable dataset) ie an immutable collection that can be processed in clusture.
Temporary tables

%spark2
val driverMileage_temp1=hiveContext.sql("select * from drivermileage")

%spark2
val geolocation_temp1=hiveContext.sql("select * from geolocation")

Registering temp tables now

geolocation_temp1.createOrReplaceTempView("geolocation_temp1")
driverMileage_temp1.createOrReplaceTempView("drivermileage_temp1")
hiveContext.sql("show tables").show()

we will perform an iteration and a filter operation. First, 
we need to filter drivers that have non-normal events associated with 
them and then count the number for non-normal events for each driver.

val geolocation_temp2 = hiveContext.sql("SELECT driverid, count(driverid) occurance from geolocation_temp1 where event!='normal' group by driverid")

register this temp table
geolocation_temp2.createOrReplaceTempView("geolocation_temp2")
hiveContext.sql("show tables").show()

Following is an action operation on RDD

geolocation_temp2.show(10) it will give us results back as list with top 10 rows

Now we will do an join operation
we will perform a join operation geolocation_temp2 table has details of drivers and count of their respective non-normal events.
drivermileage_temp1 table has details of total miles travelled by each driver.
We will join two tables on common column, which in our case is driverid

val joined = hiveContext.sql("select a.driverid,a.occurance,b.totmiles from geolocation_temp2 a,drivermileage_temp1 b where a.driverid=b.driverid")


The resulting data set will give us total miles and total non-normal events for a particular driver. 
Register this filtered table as a temporary table so that subsequent SQL queries can be applied to it.

joined.createOrReplaceTempView("joined")
hiveContext.sql("show tables").show()


joined.show(10)

we will associate a driver risk factor with every driver. 
Driver risk factor will be calculated by dividing total miles travelled by non-normal event occurrences.

val risk_factor_spark = hiveContext.sql("select driverid, occurance, totmiles, totmiles/occurance riskfactor from joined")

risk_factor_spark.createOrReplaceTempView("risk_factor_spark")
hiveContext.sql("show tables").show()

Now we will save this data back to hive in ORC format

hiveContext.sql("create table finalresults( driverid String, occurance bigint, totmiles bigint, riskfactor double) stored as orc").toDF()
hiveContext.sql("show tables").show()   

toDF() creates a DataFrame with columns driverid String, occurance bigint, etc.

Before we load the data into hive table that we created above, 
we will have to convert our data file into ORC format too

risk_factor_spark.write.format("orc").save("risk_factor_spark")


Now LOAD THE DATA INTO HIVE TABLE USING LOAD DATA COMMAND

hiveContext.sql("load data inpath 'risk_factor_spark' into table finalresults")

CREATE THE FINAL TABLE RISKFACTOR USING CTAS
hiveContext.sql("create table riskfactor as select * from finalresults").toDF()


%jdbc(hive)
SELECT * FROM riskfactor  ---> to view risk factor data and click on charts icon

%jdbc(hive)
SELECT a.driverid, a.riskfactor, b.city, b.state
FROM riskfactor a, geolocation b where a.driverid=b.driverid


spark on python use spark-submit nameoffile.python
send data to sandbox
	scp -P 2222 ~/Downloads/HDF-1.2.0.1-1.tar.gz root@localhost:/root
	scp -P <input-port> </input-directory-path-local-mach> <input-username@hostname-:/sandbox-dir-path>
send data from sandbox

	scp -P <input-port> <input-username@hostname-:/sandbox-dir-path> </input-directory-path-local-mach>


Setting up flume

login into maria dev
cd /usr/hdp/current/flume-server
bin/flume-ng agent --conf conf --conf-file ~/sparksteaming.conf --name a1 -Dflume.root.logger=INFO,console

test telnet localhost 44444

exit telnet using cntrl+ ] and then type quit

using streaming with flume
export SPARK_MAJOR_VERION=2
spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.0.0 SparkFlume.py    --->>supplying streaming connector for spark 





INSTALLING CASANDRA on SANDBOX
it needs python 2.7

yum update
yum install scl-utils
yum install centos-release-scl-rh
yum install python27

change to python 27 run following comand
scl enable python27 bash

create a repo file in /etc/yum.repos.d/
name it datastax.repo
 [datastax]
 name=datastax repo for casandra
 basurl=http://rpm.datastax.com/community
 enable=1
 gpgcheck=0

yum clean all or yum clean metadata in case yum  not working

yum install dsc30
yum install cassandra30-tools

use command nodetool status -->getting status of node

pip install cqlsh  it is tool used to query casandra
if pip is not installed then install it using command easy_install pip
Now casandra is installed on sandbox
run casandra using following commands

service cassandra start

cqlsh --cqlversion="3.4.0"

now in cql prompt
	first create keyspace for database
	CREATE KEYSPACE movielens WITH replication={'class':'SimpleStrategy','replication_factor':'1'} AND durable_writes=true;
	USE movielens

	Now we will create a table
	CREATE TABLE users (user_id int,age int,gender text,occupation text,zip text,PRIMARY KEY (user_id));
	DESCRIBE TABLE users
	SELECT * FROM users

Key space in cassandra is like database schema in normal db

running cassandra we need to provide connector location for cassandra connector for spark 

spark-submit --packages datastax:spark-cassandra-connector:2.0.0-M2-s_2.11 CassandraSpark.py
export SPARK_MAJOR_VERSION=2
spark-submit  --packages datastax:spark-cassandra-connector:2.0.0-M2-s_2.11,org.apache.spark:spark-streaming-flume_2.11:2.0.0 SparkFlume.py
if address bind exception then use 
netstat -lnap to get all ports and pid and then find 4040 and kill process using kill -9 pid then try again